{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Collective Intelligence Week 7: Information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: \n",
    "\n",
    "__Student id(s)__ : \n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file.\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import math\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1** Make a couple of functions which do the following:\n",
    "\n",
    "1. Download all these files: <http://www.ibiblio.org/xml/examples/shakespeare/>, and store them locally in a folder.\n",
    "2. Make a function `index_collection(folder)` which reads in each file `f` in that folder, and returns a dict with words as keys and as values a dict with filenames as keys and how often the word occurs in the filename as values. So:\n",
    "    1. Extract the text\n",
    "    3. tokenize and lower case that text\n",
    "    2. for each token `w`: how often `w` occurs in `f`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_my_folder='shakespeare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    # your code here\n",
    "    # initialize MyIndex\n",
    "    MyIndex = {}\n",
    "    # loop over each file\n",
    "    files = [f for f in os.listdir() if '.txt' in f]\n",
    "    N = len(files)\n",
    "    MyIndex['files'] = files\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            fd = nltk.FreqDist(nltk.tokenize.word_tokenize(f.read().lower()))\n",
    "        for w, c in fd.items():\n",
    "            w = MyIndex.get(w)\n",
    "            if w:\n",
    "                w.update({f:c})\n",
    "                w['df'] = (w['df'] * N + 1) / N\n",
    "            else:\n",
    "                MyIndex[w] = {f:c, 'df': 1 / N}\n",
    "        # open file\n",
    "        # get the text out\n",
    "        # tokenize, lower case\n",
    "        # update MyIndex with each token \n",
    "    return MyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try it out \n",
    "shakespeare= index_collection(path_to_my_folder)\n",
    "\n",
    "# Now I can call \n",
    "shakespeare['love']\n",
    "# and obtain a dict of filename:count pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**  Change your function `index_collection(folder)` so that it also stores the document frequency of each word: as follows\n",
    "\n",
    "```\n",
    "shakespeare['macbeth'] = (document_frequency of 'macbeth', dict of filename:count pairs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shakespeare=index_collection_plusDF(path_to_my_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shakespeare['love']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** Create a function `query(index,string)` which does information retrieval on a created index.\n",
    "\n",
    "1. It first tokenizes string into a list `L` of keywords .\n",
    "2. Then it computes `sum(score(w,doc) for w in L)` for each document `doc` in our collection.\n",
    "3. `score(w,doc)` is simply `(TF(w,doc) * IDF(w)) `, where `TF(w,doc)` is the raw frequency of `w` in `doc` and `IDF(w)` is the inverse document frequency as defined in [G. Adomavicius and A. Tuzhilin. \"Towards the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions.\" IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, June 2005. ](http://web.stanford.edu/class/ee378b/papers/adomavicius-recsys.pdf) \n",
    "4. The output is a list of `(score,filename)` pairs sorted on `score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer\n",
    "\n",
    "def score(w,doc,index,N):\n",
    "    # your code\n",
    "    TF=  \n",
    "    IDF =  \n",
    "    return TF*IDF\n",
    "    \n",
    "    \n",
    "    \n",
    "def query(index,string):\n",
    "    # set the list of docs and the number of docs\n",
    "    Docs=   \n",
    "    Number_of_docs=  \n",
    "    # step 1 tokenize string\n",
    "    \n",
    "    # compute score for each doc\n",
    "    \n",
    "    # sort\n",
    "     return\n",
    "    \n",
    "# test\n",
    "query(shakespeare,'caesar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4** Argue that you have just implemented the dot-product of the query-vector and the document-vector. \n",
    "\n",
    "* Experiment with adding the same term multiple times to the query. What happens? Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "\n",
    "# Create a typical power law directed network\n",
    "H= nx.gnc_graph(10)\n",
    "\n",
    "nx.draw(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute pagerank\n",
    "% time nx.pagerank(H).values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some handy functions\n",
    "\n",
    "n=4\n",
    "H.out_degree(n), H.neighbors(n),H.edges(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in H.nodes():\n",
    "    print H.neighbors(n),H.out_degree(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Return all nodes which point to n\n",
    "def looks_at_me(H,n):\n",
    "    return [k for (k,x) in H.edges() if x==n]\n",
    "\n",
    "for n in H.nodes():\n",
    "    if not looks_at_me(H,n)==[]:\n",
    "        print n, looks_at_me(H,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5** Use the iterative pagerank calculation in <http://en.wikipedia.org/wiki/PageRank#Iterative>\n",
    "  to create a function `my_pagerank(H,iterations=20)` which calculates the pagerank of each node. The output is, just as with `nx.pagerank()` a dict with nodes as keys and pageranks as values.\n",
    "\n",
    "There are several definitions of PageRank. The most useful is when we can view the distribution of PageRank over the nodes as a probability distribution. This means the values of all nodes have to sum to 1.\n",
    "\n",
    "So after computing the PageRanks, normalize the values, so that they sum to one. The [wikipedia article on pagerank](http://en.wikipedia.org/wiki/PageRank) also makes this point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer\n",
    "\n",
    "def my_pagerank2(H,iterations=20,damping=.85):\n",
    "    # Initialize every node with a pagerank of 1/|nodes| \n",
    "    # your code\n",
    "    # loop over all iterations\n",
    "    \n",
    "    # normalize\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% time my_pagerank2(H).items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**\n",
    "Compare your result and that of networkx  by calculating the maximum squared error (= difference) over the nodes. You may even investigate the effect of the number of iterations on rather large networks.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7** Compute the person correlation between your result and that of networkx.\n",
    "* HINT: in the notebook <http://nbviewer.ipython.org/url/maartenmarx.nl/teaching/CollectieveIntelligentie/NoteBooks/InformationRetrieval.ipynb> we created a pandas series from a list, but you can just as easily in the same way create a series from a dict. Then use pandas correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8** Compare your, hopefully very declarative and short (it can be done in 5 lines of readable code) definition of pagerank with that in the Collective Intelligence book, on page 71. \n",
    "\n",
    "* Do you obtain different values?\n",
    "* Compare readability.\n",
    "* Why would you want to store this information in a database while doing the computation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
