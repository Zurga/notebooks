{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calculation\n",
    "\n",
    "##Euclidian Distance\n",
    "\n",
    "$$\\|P\\|= n \\in P_{n}\\sqrt{p_{n1}^2+p_{n2}^2+\\cdots +p_{n\\dotso}^2} = \\sqrt{p \\cdot p}$$\n",
    "\n",
    "###Calculating with one vector:\n",
    "$$\\langle a, b, c \\rangle \\to \\sqrt{a^2 + b^2 + c^2}$$\n",
    "\n",
    "$$\\langle 3, 4, 5 \\rangle \\to \\sqrt{3^2 + 4^2 + 5^2}$$\n",
    "\n",
    "###Calculating with two vectors:\n",
    "$${\\langle a_{x}, a_{y}, a_{z} \\rangle, \\langle b_{x}, b_{y}, b_{z} \\rangle} \\to \\sqrt{(a_{x}-b_{x})^2 + (a_{y}-b_{y})^2 + (a_{z}-b_{z})^2} $$\n",
    "$$\\langle 2, 5, 18 \\rangle, \\langle 7, 31, 43\\rangle \\to \\sqrt{(2-7)^2 + (5-31)^2 + (18 - 43)^2}$$\n",
    "\n",
    "Because you square the differences between the vectors, you always end up with positive distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non numpy distance [3,4,5]          : 7.0710678118654755\n",
      "Non numpy distance [3,4,5], [4,5,6] : 1.7320508075688772\n",
      "With numpy distance [3,4,5]         : 7.07106781187\n",
      "With numpy distance [3,4,5], [4,5,6]: 1.73205080757\n"
     ]
    }
   ],
   "source": [
    "### Example Euclidian distance without numpy\n",
    "\n",
    "def euclidian_non_np(vector_a, vector_b=None): # We assume that we have only one vector.\n",
    "    if vector_b:  # we have two vectors == if vector_b != None\n",
    "        # zip merges two lists, like a zipper on your coat\n",
    "        distance_sums = sum((a - b)**2 for a,b in zip(vector_a, vector_b))  \n",
    "        \n",
    "        return sqrt(distance_sums)\n",
    "    \n",
    "    return sqrt(sum(a**2 for a in vector_a))\n",
    "\n",
    "## example Euclidian distance with one or two vectors using numpy\n",
    "\n",
    "def euclidian_np(a, b=None): # We assume that we have only one vector.\n",
    "    if b:  # we have two vectors\n",
    "        # function ends and return the two vectors\n",
    "        # with numpy arrays we can use minus operator to \n",
    "        # calculate the difference between each element.\n",
    "        return np.linalg.norm(np.array(a) - np.array(b)) \n",
    "    \n",
    "    return np.linalg.norm(a)  # we have only one vector\n",
    "\n",
    "print('Non numpy distance [3,4,5]          :', euclidian_non_np([3,4,5]))\n",
    "print('Non numpy distance [3,4,5], [4,5,6] :', euclidian_non_np([3,4,5],[4,5,6]))\n",
    "print('With numpy distance [3,4,5]         :', euclidian_np([3,4,5]))\n",
    "print('With numpy distance [3,4,5], [4,5,6]:', euclidian_np([3,4,5],[4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Calculation\n",
    "## Cosine Similarity\n",
    "\n",
    "$$similarity(A, B) = \\cos() = \\frac{A \\cdot B}{\\|A\\| * \\|B\\|}$$\n",
    "\n",
    "Calculating the numerator is the dot product of the two vectors. This is the same as the sum of the pairwise product of the elements in the vectors.\n",
    "\n",
    "$$\n",
    "\\langle 2, 5, 18 \\rangle\n",
    "\\langle 7, 31, 43 \\rangle \\to 2 \\cdot 7 + 5 \\cdot 31 + 18 \\cdot 43\n",
    "$$\n",
    "\n",
    "The denominator is the product of the two Euclidian distances from both vectors.\n",
    "\n",
    "$$\\langle 2, 5, 18 \\rangle, \\langle 7, 31, 43\\rangle \\to \\sqrt{(2)^2 + (5)^2 + (18)^2} * \\sqrt{(7)^2 + (31)^2 + (43)^2}$$\n",
    "\n",
    "Which results in:\n",
    "$$\\frac{2 \\cdot 7 + 5 \\cdot 31 + 18 \\cdot 43}{\\sqrt{(2)^2 + (5)^2 + (18)^2} * \\sqrt{(7)^2 + (31)^2 + (43)^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vector_a, vector_b):\n",
    "    teller = np.vdot(np.array(vector_a), np.array(vector_b)) # dot product of two vectors\n",
    "    noemer = euclidian_np(vector_a, vector_b)  # reusing the euclidian distance\n",
    "    return teller / noemer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intrinsic en extrinsic search methods\n",
    "\n",
    "##Intrinsic\n",
    "###TF * IDF\n",
    "Intrinsic methods of defining the worth of a document in terms of the query proposed to it is based around the idea that the context surrounding the document is not necessary to say something about the value of the document. This approach regards every document as a set of words with their frequency connected to it and the collection of documents as a set of the aforementioned sets.\n",
    "\n",
    "As an example you could take a collection of news articles, or all a collection of pages from Wikipedia.\n",
    "\n",
    "A method of defining the value of a document with regards to a search query Q is by multiplying the amount of times a word appears in a document (TF) with the inverse document frequency (IDF).\n",
    "\n",
    "$$TF = \\textit{amount of times a word appears in a document.}\\\\\n",
    "N = \\textit{amount of documents}\\\\\n",
    "n = \\textit{amount of document the word appears in}\\\\\n",
    "IDF = \\log{N/n}$$\n",
    "\n",
    "###Indexing\n",
    "Recalculating the TF \\* IDF by processing every file when a query is very time consuming as the collection of documents grows. Therefore it is a much better solution to store an index of the collection containing the information needed to calculate the TF \\* IDF. Re-indexing is necessary over time, and we want to extract as much information as possible from the indexing as this will reduce the calculationss necessary at query time. Below is a sample index layout based on a python dictionary:\n",
    "\n",
    "```\n",
    "index = { __filenames: [list of filenames or document names],\n",
    "                  __N: number of documents in the collection,\n",
    "                word: { doc1 : TF,\n",
    "                         doc2 : TF,\n",
    "                         doc3 : TF,\n",
    "                           DF : N / number of documents the word1 appears in},\n",
    "                word2: { doc1 : TF,\n",
    "                         doc2 : TF,\n",
    "                         doc3 : TF,\n",
    "                           DF : N / number of documents the word2 appears in}\n",
    "        }\n",
    "```\n",
    "In a real world example the index would of course be stored in a database to provide stability and redundance. Also RAM size is a limitation in terms of index size. \n",
    "\n",
    "Below is a sample implementation that will read any text file in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    '''\n",
    "    Returns an index dictionary of all tokens in all \".txt\" files\n",
    "    in the given folder.\n",
    "    >>index_collection('my_folder') \n",
    "    ''' \n",
    "    index = {}\n",
    "    folder = os.path.abspath(folder)\n",
    "    \n",
    "    # check for a trailing slash and add it if it is not there\n",
    "    if folder.endswith() != '/':\n",
    "        folder = folder + '/'\n",
    "        \n",
    "    # create list of file names and only use the files ending on .txt \n",
    "    # add the absolute path to the filename so we can use the index everywhere\n",
    "    files = [folder + f for f in os.listdir() if '.txt' in f] \n",
    "    # setting up the index\n",
    "    \n",
    "    N = len(files)\n",
    "    index['__filenames'] = files \n",
    "    index['__N'] = N\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # loop over every file to analyze the document.\n",
    "    for filename in files:\n",
    "        with open(filename) as f:\n",
    "            text = f.read().lower()  # read the whole file and lowercase it.\n",
    "            tokens = [word for word in nltk.tokenize.word_tokenize(text)\n",
    "                         if word not in stop_words]\n",
    "            fd = nltk.FreqDist(tokens)  # Generate a frequency distribution\n",
    "                                        # {word: frequency}\n",
    "        # loop over every word\n",
    "        for word, freq in fd.items():\n",
    "            idx_word = index.get(word)  # dict.get() checks if the key exists,\n",
    "                                        # it returns None if it is not in the dictionary \n",
    "            if idx_word: # it exists\n",
    "                idx_word.update({filename: freq}) # add the filename to the index\n",
    "                idx_word['df'] = (idx_word['df'] * N + 1) / N  # recalculate the DF\n",
    "            else:                       # the word is not in the index\n",
    "                index[word] = {filename : freq, 'df': 1 / N} # insert the word\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Querying and scoring\n",
    "Querying is done by entering a string of words and then summing up the TF\\*IDF score for each word in the query for each document in the collection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(word, doc, index):\n",
    "    '''\n",
    "    Returns the TFxIDF score for a given word and index.\n",
    "    '''\n",
    "    if index.get(word): # again we use the get function to prevent keyerrors\n",
    "        TF = index.get(word).get(doc, 0)  # same as index[word][doc]\n",
    "        IDF = math.log(1/index.get(word).get('df', 1))  # same as index[word]['df']\n",
    "        return TF * IDF\n",
    "    return 0 # the word was not in the document so we return a score of 0\n",
    "\n",
    "\n",
    "def query(index, qry):\n",
    "    qry = nltk.tokenize.word_tokenize(qry.lower())\n",
    "    # create a list of tuples (score sums, document)\n",
    "    score_list = [(sum(score(word, doc, index) for word in qry), doc)\n",
    "                   for doc in index['__files']]\n",
    "    # sort the document in descending order based on the scores\n",
    "    return sorted(score_list, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extrinsic\n",
    "###PageRank\n",
    "\n",
    "When calculating the relevance of a word to a document, you are using the intrinsic information of this and all other documents to do so (TF*IDF, Cosine Similarity).\n",
    "\n",
    "You can also use the extrinsic information of a collection of documents (or webpages/social network nodes) to give each document an importance score. This is not dependent on a particular query and is deducted from the collective intelligence of a dataset/corpus. We will now use the example of webpages to elaborate on this.\n",
    "\n",
    "When page A links to page B, page A basically vouches for/recommends page B. Thus, the amount of incoming links a page has can be used to calculate its general importance. This is called PageRank. Aside from the amount of incoming links a page has, PageRank also takes the importance of these incoming links into account.\n",
    "\n",
    "However, at the very beginning of starting PageRanking the pages in your collection, no page has a PageRank yet. Thus, at this starting point (t = 0), we assume all pages have the same PageRank: 1 (one) divided by the amount of pages in the collection.\n",
    "\n",
    "Example: all the pages in a collection of 5 pages don't have a PageRank yet. The score of each page is 1 / 5 = 0.2 at t = 0.\n",
    "\n",
    "From this point on, calculating the PageRank of a particular page in a collection is an iterative process, as denoted by the following formula:\n",
    "\n",
    "\n",
    "$$PR(i, t+1) = \\frac{1 - d}{N} \\ + \\ d \\ \\cdot \\ \\sum_{j \\in N(i)}\\frac{PR(j, t)}{outdegree(j)}$$\n",
    "PR(i, t+1) = \\frac{1 - d}{N} \\ + \\ d \\ \\cdot \\ \\sum_{j \\in N(i)}\\frac{PR(j, t)}{outdegree(j)}\n",
    "\n",
    "Let's take it slowly, one step at a time. To calculate the PageRank of page i:\n",
    "First, we must account for a damping factor (d). This factor, set at 0.85, prevents the formula from not working with pages without outlinks.\n",
    "We then calculate a prefix  $$\\frac{1 - d}{N}$$ which can be seen as the probability of visiting a randomly chosen page. In this prefix, N is equal to the amount of pages in the collection.\n",
    "When then take the damping factor d (remember, 0.85), and multiply it with…\n",
    "The sum (for every incoming link j page i has) of…\n",
    "The current PageRank of page j, divided by…\n",
    "The outdegree of page j (how many other pages j links to and therefore 'gives' a little bit of its PageRank to)\n",
    "\n",
    "After executing this formula over and over (remember, iterative process) for all pages until there's no change anymore in the calculated PageRank, we must normalize the then found PageRanks. This is done by dividing the PageRank of each page by the sum of all PageRanks.\n",
    "\n",
    "This table shows the growth/evolution the PageRank process for the graph below. Each node (0 … 4) can be seen as a webpage on a tiny version of the internet (our collection), consisting of 5 webpages total.\n",
    "\n",
    "\n",
    "|nodes | 0 | 1 | 2 | 3 | 4 | \n",
    "|-|---|---|---|---|---|\n",
    "| inlinks | {1, 3} | {4} | {1} | {} | {0, 1, 2, 3} |\n",
    "| outlinks | {0} | {0, 2, 4} | {4} | {0, 4} | {1} |\n",
    "| t = 0 | ⅕ = 0.20 | ⅕ = 0.20 | ⅕ = 0.20 | ⅕ = 0.20 | ⅕ = 0.20 |\n",
    "| t = 1 | 0.171666666 | 0.200000000 | 0.086666666 | 0.030000000 | 0.319000000 |\n",
    "| t = 2 | 0.099416666 | 0.301150000 | 0.115325833 | 0.030000000 | 0.310606958 |\n",
    "| t = 3 | 0.128075833 | 0.294015914 | 0.113304509 | 0.030000000 | 0.331227800 |\n",
    "| t = 4 | 0.126054509 | 0.311543630 | 0.118270695 | 0.030000000 | 0.338697118 |\n",
    "| t = 5 | 0.131020695 | 0.317892551 | 0.120069556 | 0.030000000 | 0.346246269 |\n",
    "| Normalize 0.945229072 | 0.13 / 0.95 = 0.13861263 | 0.32 / 0.95 = 0.33631271 | 0.12 / 0.95 = 0.12702693 | 0.03 / 0.95 = 0.03173833 | 0.35 / 0.95 = 0.36630937 | \n",
    "Please remember that t = 5 is not enough for accurate PageRanking. Going as far as t = 20 would be better.\n",
    "\n",
    "Here's some example calculations for PageRank:\n",
    "$$PR(page \\ 0, t1) = (\\frac{1 - 0.85}{5}) \\ + \\  (0.85 \\ \\cdot \\ (\\frac{0.20}{3} \\ + \\ \\frac{0.20}{2})) = 0.1717$$\n",
    "\n",
    "$$PR(page \\ 3, t1) = (\\frac{1 - 0.85}{5}) \\ + \\  (0.85 \\ \\cdot \\ (\\frac{0}{0})) = 0.03$$\n",
    "\n",
    "$$PR(page \\ 4, t5) = (\\frac{1 - 0.85}{5}) \\ + \\  (0.85 \\ \\cdot \\ \n",
    "(\\frac{0.126}{1} \\ + \\ \\frac{0.311}{3} \\ + \\ \\frac{0.119}{1} \\ \n",
    "+ \\ \\frac{0.030}{2})) = 0.3462$$\n",
    "\n",
    "When we implement this in Python, you'll see we'll get the same results:\n",
    "from __future__ import division # always include this, solves ZeroDivision errors\n",
    "import networkx as nx\n",
    "\n",
    "H = nx.DiGraph() # directed NetworkX network\n",
    "H.add_edges_from([(0, 4), (1, 0), (1, 2), (1, 4), (2, 4), (3, 0), (3, 4), (4, 1)]) # building up our collection/'internet'\n",
    "\n",
    "nx.pagerank(H) # inbuilt PageRank function of NetworkX\n",
    "```\n",
    ">>>{0: 0.1390711045911938,\n",
    "1: 0.33995484588171887,\n",
    "2: 0.12632110459119378,\n",
    "3: 0.030000000000000006,\n",
    "4: 0.3646529449358938}\n",
    "```\n",
    "```\n",
    "def pagerank(H, iters = 20, d = .85):\n",
    "    N = len(H)\n",
    "    prefix = (1 - d) / N\n",
    "\n",
    "    # Two ways to calculate ranks at t = 0:\n",
    "    ranks = {x: 1 / N for x in range(N + 1)}\n",
    "    ranks = dict.fromkeys(H, 1 / N)\n",
    "    \n",
    "    # For each iteration... \n",
    "    for _ in range(iters):\n",
    "        \n",
    "        # We take each page/node\n",
    "        for page in ranks:\n",
    "            \n",
    "            # We list its friends (in-links)\n",
    "            friends = [f[0] for f in H.in_edges(page)]\n",
    "            \n",
    "            # We update its rank with...\n",
    "            # The prefix + d multiplied by...\n",
    "            # The sum of, for each friend of page,\n",
    "            # It's current PageRank divided by its outdegree\n",
    "            ranks[page] = prefix + d * \\\n",
    "            sum(ranks[f] / H.out_degree(f) for f in friends)\n",
    "\n",
    "        \n",
    "    # We're done, now let's normalize all PageRanks\n",
    "    norm = sum(ranks.values())\n",
    "    return {k: ranks[k] / norm for k in ranks}\n",
    "```\n",
    "```\n",
    "pagerank(H, iters = 5)\n",
    ">>>\n",
    "{0: 0.13861263800372547,\n",
    " 1: 0.33631271023979153,\n",
    " 2: 0.1270269394649432,\n",
    " 3: 0.03173833823033561,\n",
    " 4: 0.36630937406120423}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
