{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calculation\n",
    "\n",
    "##Euclidian Distance\n",
    "\n",
    "$$\\|P\\|= n \\in P_{n}\\sqrt{p_{n1}^2+p_{n2}^2+\\cdots +p_{n\\dotso}^2} = \\sqrt{p \\cdot p}$$\n",
    "\n",
    "###Calculating with one vector:\n",
    "$$\\langle a, b, c \\rangle \\to \\sqrt{a^2 + b^2 + c^2}$$\n",
    "\n",
    "$$\\langle 3, 4, 5 \\rangle \\to \\sqrt{3^2 + 4^2 + 5^2}$$\n",
    "\n",
    "###Calculating with two vectors:\n",
    "$${\\langle a_{x}, a_{y}, a_{z} \\rangle, \\langle b_{x}, b_{y}, b_{z} \\rangle} \\to \\sqrt{(a_{x}-b_{x})^2 + (a_{y}-b_{y})^2 + (a_{z}-b_{z})^2} $$\n",
    "$$\\langle 2, 5, 18 \\rangle, \\langle 7, 31, 43\\rangle \\to \\sqrt{(2-7)^2 + (5-31)^2 + (18 - 43)^2}$$\n",
    "\n",
    "Because you square the differences between the vectors, you always end up with positive distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non numpy distance [3,4,5]          : 7.07106781187\n",
      "Non numpy distance [3,4,5], [4,5,6] : 1.73205080757\n",
      "With numpy distance [3,4,5]         : 7.07106781187\n",
      "With numpy distance [3,4,5], [4,5,6]: 1.73205080757\n"
     ]
    }
   ],
   "source": [
    "### Example Euclidian distance without numpy\n",
    "\n",
    "def euclidian_non_np(vector_a, vector_b=None): # We assume that we have only one vector.\n",
    "    if vector_b:  # we have two vectors == if vector_b != None\n",
    "        # zip merges two lists, like a zipper on your coat\n",
    "        distance_sums = sum((a - b)**2 for a,b in zip(vector_a, vector_b))  \n",
    "        \n",
    "        return sqrt(distance_sums)\n",
    "    \n",
    "    return sqrt(sum(a**2 for a in vector_a))\n",
    "\n",
    "## example Euclidian distance with one or two vectors using numpy\n",
    "\n",
    "def euclidian_np(a, b=None): # We assume that we have only one vector.\n",
    "    if b:  # we have two vectors\n",
    "        # function ends and return the two vectors\n",
    "        # with numpy arrays we can use minus operator to \n",
    "        # calculate the difference between each element.\n",
    "        return np.linalg.norm(np.array(a) - np.array(b)) \n",
    "    \n",
    "    return np.linalg.norm(a)  # we have only one vector\n",
    "\n",
    "print'Non numpy distance [3,4,5]          :', euclidian_non_np([3,4,5])\n",
    "print 'Non numpy distance [3,4,5], [4,5,6] :', euclidian_non_np([3,4,5],[4,5,6])\n",
    "print 'With numpy distance [3,4,5]         :', euclidian_np([3,4,5])\n",
    "print 'With numpy distance [3,4,5], [4,5,6]:', euclidian_np([3,4,5],[4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Calculation\n",
    "## Cosine Similarity\n",
    "\n",
    "$$similarity(A, B) = \\cos() = \\frac{A \\cdot B}{\\|A\\| * \\|B\\|}$$\n",
    "\n",
    "Calculating the numerator is the dot product of the two vectors. This is the same as the sum of the pairwise product of the elements in the vectors.\n",
    "\n",
    "$$\n",
    "\\langle 2, 5, 18 \\rangle\n",
    "\\langle 7, 31, 43 \\rangle \\to 2 \\cdot 7 + 5 \\cdot 31 + 18 \\cdot 43\n",
    "$$\n",
    "\n",
    "The denominator is the product of the two Euclidian distances from both vectors.\n",
    "\n",
    "$$\\langle 2, 5, 18 \\rangle, \\langle 7, 31, 43\\rangle \\to \\sqrt{(2)^2 + (5)^2 + (18)^2} * \\sqrt{(7)^2 + (31)^2 + (43)^2}$$\n",
    "\n",
    "Which results in:\n",
    "$$\\frac{2 \\cdot 7 + 5 \\cdot 31 + 18 \\cdot 43}{\\sqrt{(2)^2 + (5)^2 + (18)^2} * \\sqrt{(7)^2 + (31)^2 + (43)^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vector_a, vector_b):\n",
    "    teller = np.vdot(np.array(vector_a), np.array(vector_b)) # dot product of two vectors\n",
    "    noemer = euclidian_np(vector_a, vector_b)  # reusing the euclidian distance\n",
    "    return teller / noemer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intrinsic en extrinsic search methods\n",
    "\n",
    "##Intrinsic\n",
    "Intrinsic methods of defining the worth of a document in terms of the query proposed to it is based around the idea that the context surrounding the document is not necessary to say something about the value of the document. This approach regards every document as a set of words with their frequency connected to it and the collection of documents as a set of the aforementioned sets.\n",
    "\n",
    "As an example you could take a collection of news articles, or all a collection of pages from Wikipedia.\n",
    "\n",
    "A method of defining the value of a document with regards to a search query Q is by multiplying the amount of times a word appears in a document (TF) with the inverse document frequency (IDF).\n",
    "\n",
    "$$TF = \\textit{amount of times a word appears in a document.}\\\\\n",
    "N = \\textit{amount of documents}\\\\\n",
    "n = \\textit{amount of document the word appears in}\\\\\n",
    "IDF = \\log{N/n}$$\n",
    "\n",
    "###Indexing\n",
    "Recalculating the TF \\* IDF by processing every file when a query is very time consuming as the collection of documents grows. Therefore it is a much better solution to store an index of the collection containing the information needed to calculate the TF \\* IDF. Re-indexing is necessary over time, and we want to extract as much information as possible from the indexing as this will reduce the calculationss necessary at query time. Below is a sample index layout based on a python dictionary:\n",
    "\n",
    "```\n",
    "index = { __filenames: [list of filenames or document names],\n",
    "                  __N: number of documents in the collection,\n",
    "                word: { doc1 : TF,\n",
    "                         doc2 : TF,\n",
    "                         doc3 : TF,\n",
    "                           DF : N / number of documents the word1 appears in},\n",
    "                word2: { doc1 : TF,\n",
    "                         doc2 : TF,\n",
    "                         doc3 : TF,\n",
    "                           DF : N / number of documents the word2 appears in}\n",
    "        }\n",
    "```\n",
    "In a real world example the index would of course be stored in a database to provide stability and redundance. Also RAM size is a limitation in terms of index size. \n",
    "\n",
    "Below is a sample implementation that will read any text file in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_collection(folder):\n",
    "    '''\n",
    "    Returns an index dictionary of all tokens in all \".txt\" files\n",
    "    in the given folder.\n",
    "    >>index_collection('my_folder') \n",
    "    ''' \n",
    "    index = {}\n",
    "    folder = os.path.abspath(folder)\n",
    "    \n",
    "    # check for a trailing slash and add it if it is not there\n",
    "    if folder.endswith() != '/':\n",
    "        folder = folder + '/'\n",
    "        \n",
    "    # create list of file names and only use the files ending on .txt \n",
    "    # add the absolute path to the filename so we can use the index everywhere\n",
    "    files = [folder + f for f in os.listdir() if '.txt' in f] \n",
    "    # setting up the index\n",
    "    \n",
    "    N = len(files)\n",
    "    index['__filenames'] = files \n",
    "    index['__N'] = N\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # loop over every file to analyze the document.\n",
    "    for filename in files:\n",
    "        with open(filename) as f:\n",
    "            tokens = [word for word in nltk.tokenize.word_tokenize(text.lower())\n",
    "                         if word not in stop_words]\n",
    "            fd = nltk.FreqDist(tokens)  # Generate a frequency distribution\n",
    "                                        # {word: frequency}\n",
    "        # loop over every word\n",
    "        for word, freq in fd.items():\n",
    "            idx_word = index.get(word)  # dict.get() checks if the key exists,\n",
    "                                        # it returns None if it is not in the dictionary \n",
    "            if idx_word: # it exists\n",
    "                idx_word.update({filename: freq}) # add the filename to the index\n",
    "                idx_word['df'] = (idx_word['df'] * N + 1) / N  # recalculate the DF\n",
    "            else:                       # the word is not in the index\n",
    "                index[word] = {filename : freq, 'df': 1 / N} # insert the word\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Querying and scoring\n",
    "Querying is done by entering a string of words and then summing up the TF\\*IDF score for each word in the query for each document in the collection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(word, doc, index):\n",
    "    '''\n",
    "    Returns the TFxIDF score for a given word and index.\n",
    "    '''\n",
    "    if index.get(word): # again we use the get function to prevent keyerrors\n",
    "        TF = index.get(word).get(doc, 0)  # same as index[word][doc]\n",
    "        IDF = math.log(1/index.get(word).get('df', 1))  # same as index[word]['df']\n",
    "        return TF * IDF\n",
    "    return 0 # the word was not in the document so we return a score of 0\n",
    "\n",
    "\n",
    "def query(index, qry):\n",
    "    qry = nltk.tokenize.word_tokenize(qry.lower())\n",
    "    # create a list of tuples (score sums, document)\n",
    "    score_list = [(sum(score(word, doc, index) for word in qry), doc)\n",
    "                   for doc in index['__files']]\n",
    "    # sort the document in descending order based on the scores\n",
    "    return sorted(score_list, reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
